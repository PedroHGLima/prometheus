{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 2017 Tuning Analysis (v10)\n",
    "\n",
    "This notebook is dedicated to evaluated the training for the v10 tuning using data 2017 samples setted to have the same detection as the cut based in the same sample. Let's check:\n",
    "\n",
    "- Best tuning configuration;\n",
    "- Tuning efficiencies using the best configuration;\n",
    "- Training plots;\n",
    "- Roc curve;\n",
    "- And the best tuning configuration;\n",
    "\n",
    "**NOTE**: The input files is storage in: `/Volumes/castor/cern_data`\n",
    "\n",
    "**NOTE**: All output files must be storage in: `/Volumes/castor/tuning_data/Zee/v8`\n",
    "\n",
    "**NOTE**: The `cutbased`,`v7` and `v8` tuning will be used to compare the data versus monte carlo ringer tunings.\n",
    "\n",
    "## Import all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n",
      "Using all sub packages with ROOT dependence\n"
     ]
    }
   ],
   "source": [
    "from saphyra.analysis import crossval_table\n",
    "from Gaugi import load\n",
    "import os, re, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tuning file dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_op_dict(op):\n",
    "    d = {\n",
    "              op+'_pd_ref'    : \"reference/\"+op+\"_cutbased/pd_ref#0\",\n",
    "              op+'_fa_ref'    : \"reference/\"+op+\"_cutbased/fa_ref#0\",\n",
    "              op+'_sp_ref'    : \"reference/\"+op+\"_cutbased/sp_ref\",\n",
    "              op+'_pd_val'    : \"reference/\"+op+\"_cutbased/pd_val#0\",\n",
    "              op+'_fa_val'    : \"reference/\"+op+\"_cutbased/fa_val#0\",\n",
    "              op+'_sp_val'    : \"reference/\"+op+\"_cutbased/sp_val\",\n",
    "              op+'_pd_op'     : \"reference/\"+op+\"_cutbased/pd_op#0\",\n",
    "              op+'_fa_op'     : \"reference/\"+op+\"_cutbased/fa_op#0\",\n",
    "              op+'_sp_op'     : \"reference/\"+op+\"_cutbased/sp_op\",\n",
    "            \n",
    "              # Counts\n",
    "              op+'_pd_ref_passed'    : \"reference/\"+op+\"_cutbased/pd_ref#1\",\n",
    "              op+'_fa_ref_passed'    : \"reference/\"+op+\"_cutbased/fa_ref#1\",\n",
    "              op+'_pd_ref_total'     : \"reference/\"+op+\"_cutbased/pd_ref#2\",\n",
    "              op+'_fa_ref_total'     : \"reference/\"+op+\"_cutbased/fa_ref#2\",   \n",
    "              op+'_pd_val_passed'    : \"reference/\"+op+\"_cutbased/pd_val#1\",\n",
    "              op+'_fa_val_passed'    : \"reference/\"+op+\"_cutbased/fa_val#1\",\n",
    "              op+'_pd_val_total'     : \"reference/\"+op+\"_cutbased/pd_val#2\",\n",
    "              op+'_fa_val_total'     : \"reference/\"+op+\"_cutbased/fa_val#2\",  \n",
    "              op+'_pd_op_passed'     : \"reference/\"+op+\"_cutbased/pd_op#1\",\n",
    "              op+'_fa_op_passed'     : \"reference/\"+op+\"_cutbased/fa_op#1\",\n",
    "              op+'_pd_op_total'      : \"reference/\"+op+\"_cutbased/pd_op#2\",\n",
    "              op+'_fa_op_total'      : \"reference/\"+op+\"_cutbased/fa_op#2\",\n",
    "    } \n",
    "    return d\n",
    "\n",
    "tuned_info = collections.OrderedDict( {\n",
    "              # validation\n",
    "              \"max_sp_val\"      : 'summary/max_sp_val',\n",
    "              \"max_sp_pd_val\"   : 'summary/max_sp_pd_val#0',\n",
    "              \"max_sp_fa_val\"   : 'summary/max_sp_fa_val#0',\n",
    "              # Operation\n",
    "              \"max_sp_op\"       : 'summary/max_sp_op',\n",
    "              \"max_sp_pd_op\"    : 'summary/max_sp_pd_op#0',\n",
    "              \"max_sp_fa_op\"    : 'summary/max_sp_fa_op#0',\n",
    "    \n",
    "              #\"loss\"            : 'loss',\n",
    "              #\"val_loss\"        : 'val_loss',\n",
    "              #\"accuracy\"        : 'accuracy',\n",
    "              #\"val_accuracy\"    : 'val_accuracy',\n",
    "              #\"max_sp_best_epoch_val\": 'max_sp_best_epoch_val',\n",
    "              } )\n",
    "\n",
    "tuned_info.update(create_op_dict('tight'))\n",
    "tuned_info.update(create_op_dict('medium'))\n",
    "tuned_info.update(create_op_dict('loose'))\n",
    "tuned_info.update(create_op_dict('vloose'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open all tuning files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0730 01:22:07.161144 140735835288448 macros.py:23] Reading file for v7 tag from ../tuning_data/Zee/v7/*/*/*.pic.gz\n",
      "I0730 01:22:07.162093 140735835288448 macros.py:23] There are 1000 files for this task...\n",
      "I0730 01:22:07.162756 140735835288448 macros.py:23] Filling the table... \n",
      "I0730 01:22:23.825721 140735835288448 macros.py:23] End of fill step, a pandas DataFrame was created...\n",
      "I0730 01:22:24.118874 140735835288448 macros.py:23] Reading file for v8 tag from ../tuning_data/Zee/v8/*/*/*.pic.gz\n",
      "I0730 01:22:24.119894 140735835288448 macros.py:23] There are 1000 files for this task...\n",
      "I0730 01:22:24.120582 140735835288448 macros.py:23] Filling the table... \n",
      "I0730 01:22:43.972186 140735835288448 macros.py:23] End of fill step, a pandas DataFrame was created...\n",
      "I0730 01:22:44.143077 140735835288448 macros.py:23] Reading file for v10 tag from ../tuning_data/Zee/v10/*/*/*.2.pic.gz\n",
      "I0730 01:22:44.143923 140735835288448 macros.py:23] There are 500 files for this task...\n",
      "I0730 01:22:44.144577 140735835288448 macros.py:23] Filling the table... \n",
      "I0730 01:22:50.605412 140735835288448 macros.py:23] End of fill step, a pandas DataFrame was created...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-30 01:22:07,161 | Py.crossval_table                       INFO Reading file for v7 tag from ../tuning_data/Zee/v7/*/*/*.pic.gz\n",
      "2020-07-30 01:22:07,162 | Py.crossval_table                       INFO There are 1000 files for this task...\n",
      "2020-07-30 01:22:07,162 | Py.crossval_table                       INFO Filling the table... \n",
      "2020-07-30 01:22:23,825 | Py.crossval_table                       INFO End of fill step, a pandas DataFrame was created...\n",
      "2020-07-30 01:22:24,118 | Py.crossval_table                       INFO Reading file for v8 tag from ../tuning_data/Zee/v8/*/*/*.pic.gz\n",
      "2020-07-30 01:22:24,119 | Py.crossval_table                       INFO There are 1000 files for this task...\n",
      "2020-07-30 01:22:24,120 | Py.crossval_table                       INFO Filling the table... \n",
      "2020-07-30 01:22:43,972 | Py.crossval_table                       INFO End of fill step, a pandas DataFrame was created...\n",
      "2020-07-30 01:22:44,143 | Py.crossval_table                       INFO Reading file for v10 tag from ../tuning_data/Zee/v10/*/*/*.2.pic.gz\n",
      "2020-07-30 01:22:44,143 | Py.crossval_table                       INFO There are 500 files for this task...\n",
      "2020-07-30 01:22:44,144 | Py.crossval_table                       INFO Filling the table... \n",
      "2020-07-30 01:22:50,605 | Py.crossval_table                       INFO End of fill step, a pandas DataFrame was created...\n"
     ]
    }
   ],
   "source": [
    "cv_v7 = crossval_table( tuned_info )\n",
    "cv_v8 = crossval_table( tuned_info )\n",
    "cv_v10 = crossval_table( tuned_info )\n",
    "\n",
    "\n",
    "cv_v7.fill( '../tuning_data/Zee/v7/*/*/*.pic.gz', 'v7')\n",
    "cv_v8.fill( '../tuning_data/Zee/v8/*/*/*.pic.gz', 'v8')\n",
    "cv_v10.fill( '../tuning_data/Zee/v10/*/*/*.2.pic.gz', 'v10')\n",
    "#!ls ../tuning_data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep only the best inits for each sort, configuration and eta/phi bin. To calculate this we must choose an evaluation method to keep the max value in each configuration. Here, we will get the best inists looking for the max SP value for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 98)\n",
      "(2250, 98)\n",
      "(250, 98)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_inits_v7 = cv_v7.filter_inits(\"max_sp_val\")\n",
    "best_inits_v8 = cv_v8.filter_inits(\"max_sp_val\")\n",
    "best_inits_v10 = cv_v10.filter_inits(\"max_sp_val\")\n",
    "\n",
    "print(best_inits_v7.shape)\n",
    "print(best_inits_v8.shape)\n",
    "print(best_inits_v10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the reference counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fix_counts(table, op):\n",
    "    table['%s_fa_ref_total'%op]  = table['%s_fa_op_total'%op]\n",
    "    table['%s_pd_ref_total'%op]  = table['%s_pd_op_total'%op]\n",
    "    table['%s_fa_ref_passed'%op] = table['%s_fa_ref'%op] * table['%s_fa_op_total'%op]\n",
    "    table['%s_pd_ref_passed'%op] = table['%s_pd_ref'%op] * table['%s_pd_op_total'%op]\n",
    "    \n",
    "for op in ['tight','medium','loose','vloose']:\n",
    "    fix_counts(best_inits_v7, op)\n",
    "    fix_counts(best_inits_v8, op)\n",
    "    fix_counts(best_inits_v10, op)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_tag</th>\n",
       "      <th>et_bin</th>\n",
       "      <th>eta_bin</th>\n",
       "      <th>model_idx</th>\n",
       "      <th>sort</th>\n",
       "      <th>init</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tuned_idx</th>\n",
       "      <th>max_sp_val</th>\n",
       "      <th>max_sp_pd_val</th>\n",
       "      <th>...</th>\n",
       "      <th>vloose_pd_ref_total</th>\n",
       "      <th>vloose_fa_ref_total</th>\n",
       "      <th>vloose_pd_val_passed</th>\n",
       "      <th>vloose_fa_val_passed</th>\n",
       "      <th>vloose_pd_val_total</th>\n",
       "      <th>vloose_fa_val_total</th>\n",
       "      <th>vloose_pd_op_passed</th>\n",
       "      <th>vloose_fa_op_passed</th>\n",
       "      <th>vloose_pd_op_total</th>\n",
       "      <th>vloose_fa_op_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980407</td>\n",
       "      <td>0.985503</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22365</td>\n",
       "      <td>543</td>\n",
       "      <td>22625</td>\n",
       "      <td>18764</td>\n",
       "      <td>223657</td>\n",
       "      <td>5038</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979003</td>\n",
       "      <td>0.983116</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22366</td>\n",
       "      <td>619</td>\n",
       "      <td>22625</td>\n",
       "      <td>18764</td>\n",
       "      <td>223652</td>\n",
       "      <td>5299</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980362</td>\n",
       "      <td>0.984663</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22365</td>\n",
       "      <td>536</td>\n",
       "      <td>22625</td>\n",
       "      <td>18763</td>\n",
       "      <td>223652</td>\n",
       "      <td>4861</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979431</td>\n",
       "      <td>0.987138</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22365</td>\n",
       "      <td>575</td>\n",
       "      <td>22624</td>\n",
       "      <td>18764</td>\n",
       "      <td>223654</td>\n",
       "      <td>4999</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980974</td>\n",
       "      <td>0.986961</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22365</td>\n",
       "      <td>514</td>\n",
       "      <td>22624</td>\n",
       "      <td>18764</td>\n",
       "      <td>223651</td>\n",
       "      <td>5090</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_tag  et_bin  eta_bin  model_idx  sort  init  \\\n",
       "7        v10       0        0          0     0     0   \n",
       "17       v10       0        0          0     1     0   \n",
       "2        v10       0        0          0     2     1   \n",
       "15       v10       0        0          0     3     0   \n",
       "19       v10       0        0          0     4     1   \n",
       "\n",
       "                                            file_name  tuned_idx  max_sp_val  \\\n",
       "7   /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.980407   \n",
       "17  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.979003   \n",
       "2   /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.980362   \n",
       "15  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.979431   \n",
       "19  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.980974   \n",
       "\n",
       "    max_sp_pd_val  ...  vloose_pd_ref_total  vloose_fa_ref_total  \\\n",
       "7        0.985503  ...               226243               187639   \n",
       "17       0.983116  ...               226243               187639   \n",
       "2        0.984663  ...               226243               187639   \n",
       "15       0.987138  ...               226243               187639   \n",
       "19       0.986961  ...               226243               187639   \n",
       "\n",
       "    vloose_pd_val_passed  vloose_fa_val_passed  vloose_pd_val_total  \\\n",
       "7                  22365                   543                22625   \n",
       "17                 22366                   619                22625   \n",
       "2                  22365                   536                22625   \n",
       "15                 22365                   575                22624   \n",
       "19                 22365                   514                22624   \n",
       "\n",
       "    vloose_fa_val_total  vloose_pd_op_passed  vloose_fa_op_passed  \\\n",
       "7                 18764               223657                 5038   \n",
       "17                18764               223652                 5299   \n",
       "2                 18763               223652                 4861   \n",
       "15                18764               223654                 4999   \n",
       "19                18764               223651                 5090   \n",
       "\n",
       "    vloose_pd_op_total  vloose_fa_op_total  \n",
       "7               226243              187639  \n",
       "17              226243              187639  \n",
       "2               226243              187639  \n",
       "15              226243              187639  \n",
       "19              226243              187639  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_inits_v10.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the evolution for each configuration\n",
    "\n",
    "Here, each configurations means the number of neurons in the hidden layer. The tuning proceeding follow this topology:\n",
    "\n",
    "- First layer with 100 inputs;\n",
    "- Second layer with 2 to 10 neurons (all with hyperbolic tangent);\n",
    "- One output with hyperbolic tangent.\n",
    "\n",
    "Let's check the SP, Fake and detection evolution for each layer configuration calculating the mean and std values for all 10 best inits (1 init per sort, 10 sorts, 10 values per configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving sp_evolution_per_config_et0_eta0.pdf...\n",
      "saving sp_evolution_per_config_et0_eta1.pdf...\n",
      "saving sp_evolution_per_config_et0_eta2.pdf...\n",
      "saving sp_evolution_per_config_et0_eta3.pdf...\n",
      "saving sp_evolution_per_config_et0_eta4.pdf...\n",
      "saving sp_evolution_per_config_et1_eta0.pdf...\n",
      "saving sp_evolution_per_config_et1_eta1.pdf...\n",
      "saving sp_evolution_per_config_et1_eta2.pdf...\n",
      "saving sp_evolution_per_config_et1_eta3.pdf...\n",
      "saving sp_evolution_per_config_et1_eta4.pdf...\n",
      "saving sp_evolution_per_config_et2_eta0.pdf...\n",
      "saving sp_evolution_per_config_et2_eta1.pdf...\n",
      "saving sp_evolution_per_config_et2_eta2.pdf...\n",
      "saving sp_evolution_per_config_et2_eta3.pdf...\n",
      "saving sp_evolution_per_config_et2_eta4.pdf...\n",
      "saving sp_evolution_per_config_et3_eta0.pdf...\n",
      "saving sp_evolution_per_config_et3_eta1.pdf...\n",
      "saving sp_evolution_per_config_et3_eta2.pdf...\n",
      "saving sp_evolution_per_config_et3_eta3.pdf...\n",
      "saving sp_evolution_per_config_et3_eta4.pdf...\n",
      "saving sp_evolution_per_config_et4_eta0.pdf...\n",
      "saving sp_evolution_per_config_et4_eta1.pdf...\n",
      "saving sp_evolution_per_config_et4_eta2.pdf...\n",
      "saving sp_evolution_per_config_et4_eta3.pdf...\n",
      "saving sp_evolution_per_config_et4_eta4.pdf...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_evolution( t , et_bin, eta_bin , output, display=False):\n",
    "\n",
    "    sp_val_mean = []; sp_val_std = []\n",
    "    for model_idx in t.model_idx.unique():\n",
    "        table = t.loc[ (t.model_idx==model_idx) & (t.et_bin==et_bin) & (t.eta_bin==eta_bin)]    \n",
    "        sp_val_mean.append( table['max_sp_val'].mean() * 100)\n",
    "        sp_val_std.append( table['max_sp_val'].std() * 100)\n",
    "    neurons = [ i+2 for i in range(len(sp_val_mean))]\n",
    "    fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
    "    plt.errorbar(neurons, sp_val_mean, yerr=sp_val_std, label='Max SP (validation)')\n",
    "    ax.set(xlabel='#neurons', ylabel='SP',\n",
    "       title='Max SP in the validation set (et=%d, eta=%d)'%(et_bin,eta_bin))\n",
    "    ax.grid()\n",
    "    print('saving %s...'%output)\n",
    "    plt.savefig(output)\n",
    "    if display:\n",
    "        plt.show()      \n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "        \n",
    "for et_bin in best_inits_v10.et_bin.unique():\n",
    "    for eta_bin in best_inits_v10.eta_bin.unique():\n",
    "        plot_evolution(best_inits_v10, et_bin, eta_bin, 'sp_evolution_per_config_et%d_eta%d.pdf'%(et_bin,eta_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's keep the five neurons (model id = 3) in the hidden layer for all phase spaces.\n",
    "\n",
    "## Calculate the cross validation table for all phase spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 98)\n",
      "(250, 98)\n",
      "(250, 98)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_tag</th>\n",
       "      <th>et_bin</th>\n",
       "      <th>eta_bin</th>\n",
       "      <th>model_idx</th>\n",
       "      <th>sort</th>\n",
       "      <th>init</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tuned_idx</th>\n",
       "      <th>max_sp_val</th>\n",
       "      <th>max_sp_pd_val</th>\n",
       "      <th>...</th>\n",
       "      <th>vloose_pd_ref_total</th>\n",
       "      <th>vloose_fa_ref_total</th>\n",
       "      <th>vloose_pd_val_passed</th>\n",
       "      <th>vloose_fa_val_passed</th>\n",
       "      <th>vloose_pd_val_total</th>\n",
       "      <th>vloose_fa_val_total</th>\n",
       "      <th>vloose_pd_op_passed</th>\n",
       "      <th>vloose_fa_op_passed</th>\n",
       "      <th>vloose_pd_op_total</th>\n",
       "      <th>vloose_fa_op_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.978133</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>...</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "      <td>486</td>\n",
       "      <td>1047</td>\n",
       "      <td>492</td>\n",
       "      <td>33247</td>\n",
       "      <td>4860</td>\n",
       "      <td>14381</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.978463</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>...</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "      <td>486</td>\n",
       "      <td>1572</td>\n",
       "      <td>492</td>\n",
       "      <td>33247</td>\n",
       "      <td>4860</td>\n",
       "      <td>14867</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.975490</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>...</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "      <td>486</td>\n",
       "      <td>1683</td>\n",
       "      <td>492</td>\n",
       "      <td>33247</td>\n",
       "      <td>4860</td>\n",
       "      <td>15954</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971869</td>\n",
       "      <td>0.973577</td>\n",
       "      <td>...</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "      <td>486</td>\n",
       "      <td>2315</td>\n",
       "      <td>492</td>\n",
       "      <td>33247</td>\n",
       "      <td>4860</td>\n",
       "      <td>14499</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.977755</td>\n",
       "      <td>0.993902</td>\n",
       "      <td>...</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "      <td>486</td>\n",
       "      <td>1209</td>\n",
       "      <td>492</td>\n",
       "      <td>33246</td>\n",
       "      <td>4860</td>\n",
       "      <td>14605</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_tag  et_bin  eta_bin  model_idx  sort  init  \\\n",
       "162        v7       0        0          3     0     0   \n",
       "79         v7       0        0          3     1     0   \n",
       "125        v7       0        0          3     2     0   \n",
       "148        v7       0        0          3     3     1   \n",
       "111        v7       0        0          3     4     1   \n",
       "\n",
       "                                             file_name  tuned_idx  max_sp_val  \\\n",
       "162  /Users/jodafons/Desktop/phd_local/prometheus/a...          3    0.978133   \n",
       "79   /Users/jodafons/Desktop/phd_local/prometheus/a...          3    0.978463   \n",
       "125  /Users/jodafons/Desktop/phd_local/prometheus/a...          3    0.975490   \n",
       "148  /Users/jodafons/Desktop/phd_local/prometheus/a...          3    0.971869   \n",
       "111  /Users/jodafons/Desktop/phd_local/prometheus/a...          3    0.977755   \n",
       "\n",
       "     max_sp_pd_val  ...  vloose_pd_ref_total  vloose_fa_ref_total  \\\n",
       "162       0.987805  ...                 4916               332468   \n",
       "79        0.983740  ...                 4916               332468   \n",
       "125       0.983740  ...                 4916               332468   \n",
       "148       0.973577  ...                 4916               332468   \n",
       "111       0.993902  ...                 4916               332468   \n",
       "\n",
       "     vloose_pd_val_passed  vloose_fa_val_passed  vloose_pd_val_total  \\\n",
       "162                   486                  1047                  492   \n",
       "79                    486                  1572                  492   \n",
       "125                   486                  1683                  492   \n",
       "148                   486                  2315                  492   \n",
       "111                   486                  1209                  492   \n",
       "\n",
       "     vloose_fa_val_total  vloose_pd_op_passed  vloose_fa_op_passed  \\\n",
       "162                33247                 4860                14381   \n",
       "79                 33247                 4860                14867   \n",
       "125                33247                 4860                15954   \n",
       "148                33247                 4860                14499   \n",
       "111                33246                 4860                14605   \n",
       "\n",
       "     vloose_pd_op_total  vloose_fa_op_total  \n",
       "162                4916              332468  \n",
       "79                 4916              332468  \n",
       "125                4916              332468  \n",
       "148                4916              332468  \n",
       "111                4916              332468  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_inits_v7 = best_inits_v7.loc[(best_inits_v7.model_idx==3)]\n",
    "best_inits_v8 = best_inits_v8.loc[(best_inits_v8.model_idx==3)]\n",
    "best_inits_v10 = best_inits_v10.loc[(best_inits_v10.model_idx==0)]\n",
    "\n",
    "\n",
    "print(best_inits_v7.shape)\n",
    "print(best_inits_v8.shape)\n",
    "print(best_inits_v10.shape)\n",
    "\n",
    "best_inits = pd.concat([best_inits_v7, best_inits_v8, best_inits_v10])\n",
    "best_inits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 98)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 25 bins X 10 sorts = 250 rows X 2 (v7 and v8)\n",
    "best_inits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_tag</th>\n",
       "      <th>et_bin</th>\n",
       "      <th>eta_bin</th>\n",
       "      <th>max_sp_val_mean</th>\n",
       "      <th>max_sp_val_std</th>\n",
       "      <th>max_sp_pd_val_mean</th>\n",
       "      <th>max_sp_pd_val_std</th>\n",
       "      <th>max_sp_fa_val_mean</th>\n",
       "      <th>max_sp_fa_val_std</th>\n",
       "      <th>max_sp_op_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>vloose_pd_ref_total</th>\n",
       "      <th>vloose_fa_ref_total</th>\n",
       "      <th>vloose_pd_val_total_mean</th>\n",
       "      <th>vloose_pd_val_total_std</th>\n",
       "      <th>vloose_fa_val_total_mean</th>\n",
       "      <th>vloose_fa_val_total_std</th>\n",
       "      <th>vloose_pd_op_total_mean</th>\n",
       "      <th>vloose_pd_op_total_std</th>\n",
       "      <th>vloose_fa_op_total_mean</th>\n",
       "      <th>vloose_fa_op_total_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977106</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.986779</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.004407</td>\n",
       "      <td>0.974279</td>\n",
       "      <td>...</td>\n",
       "      <td>4916</td>\n",
       "      <td>332468</td>\n",
       "      <td>491.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>33246.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>4916.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>332468.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967836</td>\n",
       "      <td>0.004014</td>\n",
       "      <td>0.980051</td>\n",
       "      <td>0.007838</td>\n",
       "      <td>0.044288</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>0.963947</td>\n",
       "      <td>...</td>\n",
       "      <td>3008</td>\n",
       "      <td>253818</td>\n",
       "      <td>300.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>25381.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>3008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253818.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.948442</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>0.967991</td>\n",
       "      <td>0.016779</td>\n",
       "      <td>0.070815</td>\n",
       "      <td>0.016083</td>\n",
       "      <td>0.938309</td>\n",
       "      <td>...</td>\n",
       "      <td>1155</td>\n",
       "      <td>48521</td>\n",
       "      <td>115.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>4852.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48521.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.962749</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.976954</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.959627</td>\n",
       "      <td>...</td>\n",
       "      <td>4555</td>\n",
       "      <td>331654</td>\n",
       "      <td>455.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>33165.4</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>4555.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331654.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.967934</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>0.987278</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>0.051154</td>\n",
       "      <td>0.015170</td>\n",
       "      <td>0.954410</td>\n",
       "      <td>...</td>\n",
       "      <td>472</td>\n",
       "      <td>40642</td>\n",
       "      <td>47.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>4064.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>472.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40642.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>v7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984498</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.989059</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.020052</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.984097</td>\n",
       "      <td>...</td>\n",
       "      <td>32722</td>\n",
       "      <td>168165</td>\n",
       "      <td>3272.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>16816.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>32722.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168165.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978056</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.986244</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.030091</td>\n",
       "      <td>0.003898</td>\n",
       "      <td>0.977335</td>\n",
       "      <td>...</td>\n",
       "      <td>19264</td>\n",
       "      <td>129473</td>\n",
       "      <td>1926.4</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>12947.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>19264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129473.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.959119</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.976630</td>\n",
       "      <td>0.009583</td>\n",
       "      <td>0.058194</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.956317</td>\n",
       "      <td>...</td>\n",
       "      <td>6033</td>\n",
       "      <td>21806</td>\n",
       "      <td>603.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>2180.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>6033.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21806.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>v7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972069</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.983663</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.039452</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.971316</td>\n",
       "      <td>...</td>\n",
       "      <td>22526</td>\n",
       "      <td>158040</td>\n",
       "      <td>2252.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>15804.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22526.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158040.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>v7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.962842</td>\n",
       "      <td>0.004688</td>\n",
       "      <td>0.982429</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>0.056515</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.956809</td>\n",
       "      <td>...</td>\n",
       "      <td>2162</td>\n",
       "      <td>19039</td>\n",
       "      <td>216.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>1903.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>2162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19039.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>v7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990633</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.990315</td>\n",
       "      <td>...</td>\n",
       "      <td>87257</td>\n",
       "      <td>34622</td>\n",
       "      <td>8725.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>3462.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>87257.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34622.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>v7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985501</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.991481</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.020460</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.985050</td>\n",
       "      <td>...</td>\n",
       "      <td>53175</td>\n",
       "      <td>27420</td>\n",
       "      <td>5317.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>2742.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27420.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>v7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.968320</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.976630</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.039927</td>\n",
       "      <td>0.009186</td>\n",
       "      <td>0.966438</td>\n",
       "      <td>...</td>\n",
       "      <td>13949</td>\n",
       "      <td>5735</td>\n",
       "      <td>1394.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>573.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>13949.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5735.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>v7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.980380</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.991001</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.030182</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.979993</td>\n",
       "      <td>...</td>\n",
       "      <td>48453</td>\n",
       "      <td>30217</td>\n",
       "      <td>4845.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>3021.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>48453.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30217.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>v7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.970844</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.984111</td>\n",
       "      <td>0.010186</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.012094</td>\n",
       "      <td>0.965805</td>\n",
       "      <td>...</td>\n",
       "      <td>4217</td>\n",
       "      <td>3333</td>\n",
       "      <td>421.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>333.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>4217.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>v7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.995703</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.008896</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.992303</td>\n",
       "      <td>...</td>\n",
       "      <td>95638</td>\n",
       "      <td>10229</td>\n",
       "      <td>9563.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>1022.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>95638.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10229.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>v7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.989562</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.993554</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.014419</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.988219</td>\n",
       "      <td>...</td>\n",
       "      <td>60035</td>\n",
       "      <td>8045</td>\n",
       "      <td>6003.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>804.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>60035.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8045.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>v7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.964220</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>0.977804</td>\n",
       "      <td>0.010657</td>\n",
       "      <td>0.049225</td>\n",
       "      <td>0.012616</td>\n",
       "      <td>0.956663</td>\n",
       "      <td>...</td>\n",
       "      <td>11128</td>\n",
       "      <td>1788</td>\n",
       "      <td>1112.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>178.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>11128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>v7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.985599</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.993554</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.022314</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>0.984646</td>\n",
       "      <td>...</td>\n",
       "      <td>53986</td>\n",
       "      <td>8111</td>\n",
       "      <td>5398.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>811.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>53986.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8111.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>v7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.982267</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>0.987509</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>0.022921</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>0.975828</td>\n",
       "      <td>...</td>\n",
       "      <td>4485</td>\n",
       "      <td>829</td>\n",
       "      <td>448.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>82.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>v7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995292</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.997202</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>...</td>\n",
       "      <td>35377</td>\n",
       "      <td>5141</td>\n",
       "      <td>3537.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>514.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>35377.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5141.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>v7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990465</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.993585</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.987620</td>\n",
       "      <td>...</td>\n",
       "      <td>22447</td>\n",
       "      <td>4032</td>\n",
       "      <td>2244.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>403.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>22447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>v7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.970997</td>\n",
       "      <td>0.010071</td>\n",
       "      <td>0.979935</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.037797</td>\n",
       "      <td>0.021002</td>\n",
       "      <td>0.962623</td>\n",
       "      <td>...</td>\n",
       "      <td>4038</td>\n",
       "      <td>978</td>\n",
       "      <td>403.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>97.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>4038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>v7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987131</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.993498</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.984524</td>\n",
       "      <td>...</td>\n",
       "      <td>19224</td>\n",
       "      <td>3437</td>\n",
       "      <td>1922.4</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>343.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>19224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3437.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>v7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.985447</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>0.986689</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.015726</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>0.970751</td>\n",
       "      <td>...</td>\n",
       "      <td>1501</td>\n",
       "      <td>317</td>\n",
       "      <td>150.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>v8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.976662</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.984137</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.030783</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.976458</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22624.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>18763.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>226243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187639.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>v8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967618</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.979159</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.043848</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.967296</td>\n",
       "      <td>...</td>\n",
       "      <td>136848</td>\n",
       "      <td>143657</td>\n",
       "      <td>13684.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>14365.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>136848.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143657.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>v8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.935651</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.941871</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.070513</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.934722</td>\n",
       "      <td>...</td>\n",
       "      <td>50009</td>\n",
       "      <td>30037</td>\n",
       "      <td>5000.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>3003.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>50009.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30037.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>v8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.964765</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.979564</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.049919</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.964518</td>\n",
       "      <td>...</td>\n",
       "      <td>220933</td>\n",
       "      <td>205792</td>\n",
       "      <td>22093.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>20579.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>220933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205792.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>v8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.946672</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.951578</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.058212</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.945019</td>\n",
       "      <td>...</td>\n",
       "      <td>19330</td>\n",
       "      <td>15512</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1551.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>19330.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>v8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995985</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.997812</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.995958</td>\n",
       "      <td>...</td>\n",
       "      <td>1650658</td>\n",
       "      <td>3769656</td>\n",
       "      <td>165065.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>376965.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1650658.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3769656.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>v8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992838</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.996084</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>...</td>\n",
       "      <td>1029912</td>\n",
       "      <td>2560472</td>\n",
       "      <td>102991.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>256047.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>1029912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2560472.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>v8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.986319</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.988576</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.015935</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.986173</td>\n",
       "      <td>...</td>\n",
       "      <td>165962</td>\n",
       "      <td>560953</td>\n",
       "      <td>16596.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>56095.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>165962.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>560953.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>v8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987811</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.994963</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.987753</td>\n",
       "      <td>...</td>\n",
       "      <td>875458</td>\n",
       "      <td>1318418</td>\n",
       "      <td>87545.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>131841.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>875458.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1318418.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>v8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.989566</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.994178</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.015034</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.989269</td>\n",
       "      <td>...</td>\n",
       "      <td>87254</td>\n",
       "      <td>93785</td>\n",
       "      <td>8725.4</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>9378.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>87254.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93785.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980243</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.986011</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.025506</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.981054</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22624.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>18763.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>226243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187639.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971924</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.979554</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.035675</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.973599</td>\n",
       "      <td>...</td>\n",
       "      <td>136848</td>\n",
       "      <td>143657</td>\n",
       "      <td>13684.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>14365.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>136848.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143657.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.947348</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.953348</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.058628</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.948506</td>\n",
       "      <td>...</td>\n",
       "      <td>50009</td>\n",
       "      <td>30037</td>\n",
       "      <td>5000.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>3003.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>50009.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30037.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972486</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.983285</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.038252</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.973831</td>\n",
       "      <td>...</td>\n",
       "      <td>220933</td>\n",
       "      <td>205792</td>\n",
       "      <td>22093.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>20579.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>220933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205792.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.948216</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.953337</td>\n",
       "      <td>0.010613</td>\n",
       "      <td>0.056859</td>\n",
       "      <td>0.006922</td>\n",
       "      <td>0.946505</td>\n",
       "      <td>...</td>\n",
       "      <td>19330</td>\n",
       "      <td>15512</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1551.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>19330.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15512.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988241</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.992215</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.015724</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.988786</td>\n",
       "      <td>...</td>\n",
       "      <td>1457047</td>\n",
       "      <td>316581</td>\n",
       "      <td>145704.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>31658.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>1457047.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316581.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982140</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.988821</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.982768</td>\n",
       "      <td>...</td>\n",
       "      <td>871041</td>\n",
       "      <td>227709</td>\n",
       "      <td>87104.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>22770.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>871041.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>227709.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.971063</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.978265</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.036109</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.972018</td>\n",
       "      <td>...</td>\n",
       "      <td>275404</td>\n",
       "      <td>47550</td>\n",
       "      <td>27540.4</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>4755.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>275404.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47550.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.980630</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.989357</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.028058</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.981720</td>\n",
       "      <td>...</td>\n",
       "      <td>1099898</td>\n",
       "      <td>307253</td>\n",
       "      <td>109989.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>30725.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>1099898.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307253.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.969658</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.981253</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.041865</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.969459</td>\n",
       "      <td>...</td>\n",
       "      <td>81560</td>\n",
       "      <td>36522</td>\n",
       "      <td>8156.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3652.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>81560.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36522.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993214</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.995400</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.008970</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.993477</td>\n",
       "      <td>...</td>\n",
       "      <td>4069496</td>\n",
       "      <td>223869</td>\n",
       "      <td>406949.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>22386.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>4069496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>223869.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.989145</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.993819</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.015517</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.989440</td>\n",
       "      <td>...</td>\n",
       "      <td>2466783</td>\n",
       "      <td>156992</td>\n",
       "      <td>246678.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>15699.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>2466783.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156992.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.982962</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.987752</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.983879</td>\n",
       "      <td>...</td>\n",
       "      <td>626396</td>\n",
       "      <td>35206</td>\n",
       "      <td>62639.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>3520.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>626396.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35206.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986831</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.993669</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.019984</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.987517</td>\n",
       "      <td>...</td>\n",
       "      <td>2309655</td>\n",
       "      <td>196711</td>\n",
       "      <td>230965.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>19671.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>2309655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196711.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.981369</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.991738</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.028943</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.981004</td>\n",
       "      <td>...</td>\n",
       "      <td>159644</td>\n",
       "      <td>24980</td>\n",
       "      <td>15964.4</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>159644.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24980.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994795</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.996802</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.994965</td>\n",
       "      <td>...</td>\n",
       "      <td>4495201</td>\n",
       "      <td>162421</td>\n",
       "      <td>449520.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>16242.1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>4495201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162421.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991981</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.995748</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.011779</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.992091</td>\n",
       "      <td>...</td>\n",
       "      <td>2756849</td>\n",
       "      <td>112229</td>\n",
       "      <td>275684.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>11222.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>2756849.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112229.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.985980</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.991152</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.986456</td>\n",
       "      <td>...</td>\n",
       "      <td>457627</td>\n",
       "      <td>25449</td>\n",
       "      <td>45762.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>2544.9</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>457627.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25449.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989728</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.995070</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.990077</td>\n",
       "      <td>...</td>\n",
       "      <td>2360357</td>\n",
       "      <td>131668</td>\n",
       "      <td>236035.7</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>13166.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>2360357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131668.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.986635</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.993421</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.020124</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.985755</td>\n",
       "      <td>...</td>\n",
       "      <td>182235</td>\n",
       "      <td>16100</td>\n",
       "      <td>18223.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>182235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998632</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.999199</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.998747</td>\n",
       "      <td>...</td>\n",
       "      <td>1650658</td>\n",
       "      <td>3769656</td>\n",
       "      <td>165065.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>376965.6</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>1650658.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3769656.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997566</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>...</td>\n",
       "      <td>1029912</td>\n",
       "      <td>2560472</td>\n",
       "      <td>102991.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>256047.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>1029912.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2560472.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996734</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.997174</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.997293</td>\n",
       "      <td>...</td>\n",
       "      <td>165962</td>\n",
       "      <td>560953</td>\n",
       "      <td>16596.2</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>56095.3</td>\n",
       "      <td>0.483046</td>\n",
       "      <td>165962.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>560953.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994780</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.997068</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.995232</td>\n",
       "      <td>...</td>\n",
       "      <td>875458</td>\n",
       "      <td>1318418</td>\n",
       "      <td>87545.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>131841.8</td>\n",
       "      <td>0.421637</td>\n",
       "      <td>875458.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1318418.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.993298</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.996390</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.993681</td>\n",
       "      <td>...</td>\n",
       "      <td>87254</td>\n",
       "      <td>93785</td>\n",
       "      <td>8725.4</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>9378.5</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>87254.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93785.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_tag  et_bin  eta_bin  max_sp_val_mean  max_sp_val_std  \\\n",
       "0         v7       0        0         0.977106        0.003830   \n",
       "1         v7       0        1         0.967836        0.004014   \n",
       "2         v7       0        2         0.948442        0.008567   \n",
       "3         v7       0        3         0.962749        0.003036   \n",
       "4         v7       0        4         0.967934        0.009243   \n",
       "5         v7       1        0         0.984498        0.001172   \n",
       "6         v7       1        1         0.978056        0.001465   \n",
       "7         v7       1        2         0.959119        0.002546   \n",
       "8         v7       1        3         0.972069        0.001092   \n",
       "9         v7       1        4         0.962842        0.004688   \n",
       "10        v7       2        0         0.990633        0.001007   \n",
       "11        v7       2        1         0.985501        0.001533   \n",
       "12        v7       2        2         0.968320        0.003250   \n",
       "13        v7       2        3         0.980380        0.001887   \n",
       "14        v7       2        4         0.970844        0.006767   \n",
       "15        v7       3        0         0.993401        0.001345   \n",
       "16        v7       3        1         0.989562        0.001962   \n",
       "17        v7       3        2         0.964220        0.006508   \n",
       "18        v7       3        3         0.985599        0.003245   \n",
       "19        v7       3        4         0.982267        0.009208   \n",
       "20        v7       4        0         0.995292        0.002022   \n",
       "21        v7       4        1         0.990465        0.002049   \n",
       "22        v7       4        2         0.970997        0.010071   \n",
       "23        v7       4        3         0.987131        0.006394   \n",
       "24        v7       4        4         0.985447        0.010139   \n",
       "25        v8       0        0         0.976662        0.000428   \n",
       "26        v8       0        1         0.967618        0.001110   \n",
       "27        v8       0        2         0.935651        0.002667   \n",
       "28        v8       0        3         0.964765        0.000993   \n",
       "29        v8       0        4         0.946672        0.003387   \n",
       "..       ...     ...      ...              ...             ...   \n",
       "45        v8       4        0         0.995985        0.000374   \n",
       "46        v8       4        1         0.992838        0.000588   \n",
       "47        v8       4        2         0.986319        0.001350   \n",
       "48        v8       4        3         0.987811        0.001342   \n",
       "49        v8       4        4         0.989566        0.000786   \n",
       "50       v10       0        0         0.980243        0.000634   \n",
       "51       v10       0        1         0.971924        0.000780   \n",
       "52       v10       0        2         0.947348        0.002258   \n",
       "53       v10       0        3         0.972486        0.000744   \n",
       "54       v10       0        4         0.948216        0.003582   \n",
       "55       v10       1        0         0.988241        0.000369   \n",
       "56       v10       1        1         0.982140        0.000531   \n",
       "57       v10       1        2         0.971063        0.001519   \n",
       "58       v10       1        3         0.980630        0.000628   \n",
       "59       v10       1        4         0.969658        0.001708   \n",
       "60       v10       2        0         0.993214        0.000288   \n",
       "61       v10       2        1         0.989145        0.000580   \n",
       "62       v10       2        2         0.982962        0.001211   \n",
       "63       v10       2        3         0.986831        0.000625   \n",
       "64       v10       2        4         0.981369        0.001134   \n",
       "65       v10       3        0         0.994795        0.000305   \n",
       "66       v10       3        1         0.991981        0.000513   \n",
       "67       v10       3        2         0.985980        0.001184   \n",
       "68       v10       3        3         0.989728        0.000696   \n",
       "69       v10       3        4         0.986635        0.002089   \n",
       "70       v10       4        0         0.998632        0.000038   \n",
       "71       v10       4        1         0.997566        0.000077   \n",
       "72       v10       4        2         0.996734        0.000274   \n",
       "73       v10       4        3         0.994780        0.000128   \n",
       "74       v10       4        4         0.993298        0.000614   \n",
       "\n",
       "    max_sp_pd_val_mean  max_sp_pd_val_std  max_sp_fa_val_mean  \\\n",
       "0             0.986779           0.006862            0.032511   \n",
       "1             0.980051           0.007838            0.044288   \n",
       "2             0.967991           0.016779            0.070815   \n",
       "3             0.976954           0.009870            0.051316   \n",
       "4             0.987278           0.014849            0.051154   \n",
       "5             0.989059           0.002560            0.020052   \n",
       "6             0.986244           0.004452            0.030091   \n",
       "7             0.976630           0.009583            0.058194   \n",
       "8             0.983663           0.003394            0.039452   \n",
       "9             0.982429           0.008657            0.056515   \n",
       "10            0.993548           0.001858            0.012276   \n",
       "11            0.991481           0.002059            0.020460   \n",
       "12            0.976630           0.007564            0.039927   \n",
       "13            0.991001           0.002973            0.030182   \n",
       "14            0.984111           0.010186            0.042297   \n",
       "15            0.995703           0.001327            0.008896   \n",
       "16            0.993554           0.002949            0.014419   \n",
       "17            0.977804           0.010657            0.049225   \n",
       "18            0.993554           0.004425            0.022314   \n",
       "19            0.987509           0.006423            0.022921   \n",
       "20            0.997202           0.001388            0.006613   \n",
       "21            0.993585           0.002927            0.012648   \n",
       "22            0.979935           0.014336            0.037797   \n",
       "23            0.993498           0.003470            0.019201   \n",
       "24            0.986689           0.015039            0.015726   \n",
       "25            0.984137           0.001994            0.030783   \n",
       "26            0.979159           0.004419            0.043848   \n",
       "27            0.941871           0.007527            0.070513   \n",
       "28            0.979564           0.002066            0.049919   \n",
       "29            0.951578           0.005894            0.058212   \n",
       "..                 ...                ...                 ...   \n",
       "45            0.997812           0.000362            0.005841   \n",
       "46            0.996084           0.000475            0.010404   \n",
       "47            0.988576           0.001505            0.015935   \n",
       "48            0.994963           0.000902            0.019316   \n",
       "49            0.994178           0.002138            0.015034   \n",
       "50            0.986011           0.001712            0.025506   \n",
       "51            0.979554           0.002098            0.035675   \n",
       "52            0.953348           0.004167            0.058628   \n",
       "53            0.983285           0.001914            0.038252   \n",
       "54            0.953337           0.010613            0.056859   \n",
       "55            0.992215           0.000772            0.015724   \n",
       "56            0.988821           0.000965            0.024518   \n",
       "57            0.978265           0.002750            0.036109   \n",
       "58            0.989357           0.001422            0.028058   \n",
       "59            0.981253           0.002881            0.041865   \n",
       "60            0.995400           0.000604            0.008970   \n",
       "61            0.993819           0.000546            0.015517   \n",
       "62            0.987752           0.001630            0.021814   \n",
       "63            0.993669           0.000857            0.019984   \n",
       "64            0.991738           0.001995            0.028943   \n",
       "65            0.996802           0.000712            0.007210   \n",
       "66            0.995748           0.000750            0.011779   \n",
       "67            0.991152           0.001878            0.019176   \n",
       "68            0.995070           0.000884            0.015600   \n",
       "69            0.993421           0.002462            0.020124   \n",
       "70            0.999199           0.000121            0.001934   \n",
       "71            0.998724           0.000160            0.003592   \n",
       "72            0.997174           0.000416            0.003706   \n",
       "73            0.997068           0.000413            0.007506   \n",
       "74            0.996390           0.000783            0.009788   \n",
       "\n",
       "    max_sp_fa_val_std  max_sp_op_mean  ...  vloose_pd_ref_total  \\\n",
       "0            0.004407        0.974279  ...                 4916   \n",
       "1            0.006140        0.963947  ...                 3008   \n",
       "2            0.016083        0.938309  ...                 1155   \n",
       "3            0.008947        0.959627  ...                 4555   \n",
       "4            0.015170        0.954410  ...                  472   \n",
       "5            0.001328        0.984097  ...                32722   \n",
       "6            0.003898        0.977335  ...                19264   \n",
       "7            0.009761        0.956317  ...                 6033   \n",
       "8            0.003209        0.971316  ...                22526   \n",
       "9            0.010450        0.956809  ...                 2162   \n",
       "10           0.002184        0.990315  ...                87257   \n",
       "11           0.003167        0.985050  ...                53175   \n",
       "12           0.009186        0.966438  ...                13949   \n",
       "13           0.003477        0.979993  ...                48453   \n",
       "14           0.012094        0.965805  ...                 4217   \n",
       "15           0.002894        0.992303  ...                95638   \n",
       "16           0.002998        0.988219  ...                60035   \n",
       "17           0.012616        0.956663  ...                11128   \n",
       "18           0.006295        0.984646  ...                53986   \n",
       "19           0.017462        0.975828  ...                 4485   \n",
       "20           0.004509        0.993667  ...                35377   \n",
       "21           0.003394        0.987620  ...                22447   \n",
       "22           0.021002        0.962623  ...                 4038   \n",
       "23           0.011485        0.984524  ...                19224   \n",
       "24           0.016579        0.970751  ...                 1501   \n",
       "25           0.001708        0.976458  ...               226243   \n",
       "26           0.003664        0.967296  ...               136848   \n",
       "27           0.010289        0.934722  ...                50009   \n",
       "28           0.002047        0.964518  ...               220933   \n",
       "29           0.004661        0.945019  ...                19330   \n",
       "..                ...             ...  ...                  ...   \n",
       "45           0.000473        0.995958  ...              1650658   \n",
       "46           0.000989        0.992833  ...              1029912   \n",
       "47           0.001970        0.986173  ...               165962   \n",
       "48           0.001912        0.987753  ...               875458   \n",
       "49           0.001782        0.989269  ...                87254   \n",
       "50           0.001645        0.981054  ...               226243   \n",
       "51           0.001941        0.973599  ...               136848   \n",
       "52           0.004202        0.948506  ...                50009   \n",
       "53           0.001647        0.973831  ...               220933   \n",
       "54           0.006922        0.946505  ...                19330   \n",
       "55           0.001059        0.988786  ...              1457047   \n",
       "56           0.001034        0.982768  ...               871041   \n",
       "57           0.002570        0.972018  ...               275404   \n",
       "58           0.001156        0.981720  ...              1099898   \n",
       "59           0.003603        0.969459  ...                81560   \n",
       "60           0.000677        0.993477  ...              4069496   \n",
       "61           0.001142        0.989440  ...              2466783   \n",
       "62           0.002599        0.983879  ...               626396   \n",
       "63           0.001261        0.987517  ...              2309655   \n",
       "64           0.002608        0.981004  ...               159644   \n",
       "65           0.000757        0.994965  ...              4495201   \n",
       "66           0.001200        0.992091  ...              2756849   \n",
       "67           0.002945        0.986456  ...               457627   \n",
       "68           0.001382        0.990077  ...              2360357   \n",
       "69           0.003887        0.985755  ...               182235   \n",
       "70           0.000123        0.998747  ...              1650658   \n",
       "71           0.000173        0.997767  ...              1029912   \n",
       "72           0.000356        0.997293  ...               165962   \n",
       "73           0.000535        0.995232  ...               875458   \n",
       "74           0.001179        0.993681  ...                87254   \n",
       "\n",
       "    vloose_fa_ref_total  vloose_pd_val_total_mean  vloose_pd_val_total_std  \\\n",
       "0                332468                     491.6                 0.516398   \n",
       "1                253818                     300.8                 0.421637   \n",
       "2                 48521                     115.5                 0.527046   \n",
       "3                331654                     455.5                 0.527046   \n",
       "4                 40642                      47.2                 0.421637   \n",
       "5                168165                    3272.2                 0.421637   \n",
       "6                129473                    1926.4                 0.516398   \n",
       "7                 21806                     603.3                 0.483046   \n",
       "8                158040                    2252.6                 0.516398   \n",
       "9                 19039                     216.2                 0.421637   \n",
       "10                34622                    8725.7                 0.483046   \n",
       "11                27420                    5317.5                 0.527046   \n",
       "12                 5735                    1394.9                 0.316228   \n",
       "13                30217                    4845.3                 0.483046   \n",
       "14                 3333                     421.7                 0.483046   \n",
       "15                10229                    9563.8                 0.421637   \n",
       "16                 8045                    6003.5                 0.527046   \n",
       "17                 1788                    1112.8                 0.421637   \n",
       "18                 8111                    5398.6                 0.516398   \n",
       "19                  829                     448.5                 0.527046   \n",
       "20                 5141                    3537.7                 0.483046   \n",
       "21                 4032                    2244.7                 0.483046   \n",
       "22                  978                     403.8                 0.421637   \n",
       "23                 3437                    1922.4                 0.516398   \n",
       "24                  317                     150.1                 0.316228   \n",
       "25               187639                   22624.3                 0.483046   \n",
       "26               143657                   13684.8                 0.421637   \n",
       "27                30037                    5000.9                 0.316228   \n",
       "28               205792                   22093.3                 0.483046   \n",
       "29                15512                    1933.0                 0.000000   \n",
       "..                  ...                       ...                      ...   \n",
       "45              3769656                  165065.8                 0.421637   \n",
       "46              2560472                  102991.2                 0.421637   \n",
       "47               560953                   16596.2                 0.421637   \n",
       "48              1318418                   87545.8                 0.421637   \n",
       "49                93785                    8725.4                 0.516398   \n",
       "50               187639                   22624.3                 0.483046   \n",
       "51               143657                   13684.8                 0.421637   \n",
       "52                30037                    5000.9                 0.316228   \n",
       "53               205792                   22093.3                 0.483046   \n",
       "54                15512                    1933.0                 0.000000   \n",
       "55               316581                  145704.7                 0.483046   \n",
       "56               227709                   87104.1                 0.316228   \n",
       "57                47550                   27540.4                 0.516398   \n",
       "58               307253                  109989.8                 0.421637   \n",
       "59                36522                    8156.0                 0.000000   \n",
       "60               223869                  406949.6                 0.516398   \n",
       "61               156992                  246678.3                 0.483046   \n",
       "62                35206                   62639.6                 0.516398   \n",
       "63               196711                  230965.5                 0.527046   \n",
       "64                24980                   15964.4                 0.516398   \n",
       "65               162421                  449520.1                 0.316228   \n",
       "66               112229                  275684.9                 0.316228   \n",
       "67                25449                   45762.7                 0.483046   \n",
       "68               131668                  236035.7                 0.483046   \n",
       "69                16100                   18223.5                 0.527046   \n",
       "70              3769656                  165065.8                 0.421637   \n",
       "71              2560472                  102991.2                 0.421637   \n",
       "72               560953                   16596.2                 0.421637   \n",
       "73              1318418                   87545.8                 0.421637   \n",
       "74                93785                    8725.4                 0.516398   \n",
       "\n",
       "    vloose_fa_val_total_mean  vloose_fa_val_total_std  \\\n",
       "0                    33246.8                 0.421637   \n",
       "1                    25381.8                 0.421637   \n",
       "2                     4852.1                 0.316228   \n",
       "3                    33165.4                 0.516398   \n",
       "4                     4064.2                 0.421637   \n",
       "5                    16816.5                 0.527046   \n",
       "6                    12947.3                 0.483046   \n",
       "7                     2180.6                 0.516398   \n",
       "8                    15804.0                 0.000000   \n",
       "9                     1903.9                 0.316228   \n",
       "10                    3462.2                 0.421637   \n",
       "11                    2742.0                 0.000000   \n",
       "12                     573.5                 0.527046   \n",
       "13                    3021.7                 0.483046   \n",
       "14                     333.3                 0.483046   \n",
       "15                    1022.9                 0.316228   \n",
       "16                     804.5                 0.527046   \n",
       "17                     178.8                 0.421637   \n",
       "18                     811.1                 0.316228   \n",
       "19                      82.9                 0.316228   \n",
       "20                     514.1                 0.316228   \n",
       "21                     403.2                 0.421637   \n",
       "22                      97.8                 0.421637   \n",
       "23                     343.7                 0.483046   \n",
       "24                      31.7                 0.483046   \n",
       "25                   18763.9                 0.316228   \n",
       "26                   14365.7                 0.483046   \n",
       "27                    3003.7                 0.483046   \n",
       "28                   20579.2                 0.421637   \n",
       "29                    1551.2                 0.421637   \n",
       "..                       ...                      ...   \n",
       "45                  376965.6                 0.516398   \n",
       "46                  256047.2                 0.421637   \n",
       "47                   56095.3                 0.483046   \n",
       "48                  131841.8                 0.421637   \n",
       "49                    9378.5                 0.527046   \n",
       "50                   18763.9                 0.316228   \n",
       "51                   14365.7                 0.483046   \n",
       "52                    3003.7                 0.483046   \n",
       "53                   20579.2                 0.421637   \n",
       "54                    1551.2                 0.421637   \n",
       "55                   31658.1                 0.316228   \n",
       "56                   22770.9                 0.316228   \n",
       "57                    4755.0                 0.000000   \n",
       "58                   30725.3                 0.483046   \n",
       "59                    3652.2                 0.421637   \n",
       "60                   22386.9                 0.316228   \n",
       "61                   15699.2                 0.421637   \n",
       "62                    3520.6                 0.516398   \n",
       "63                   19671.1                 0.316228   \n",
       "64                    2498.0                 0.000000   \n",
       "65                   16242.1                 0.316228   \n",
       "66                   11222.9                 0.316228   \n",
       "67                    2544.9                 0.316228   \n",
       "68                   13166.8                 0.421637   \n",
       "69                    1610.0                 0.000000   \n",
       "70                  376965.6                 0.516398   \n",
       "71                  256047.2                 0.421637   \n",
       "72                   56095.3                 0.483046   \n",
       "73                  131841.8                 0.421637   \n",
       "74                    9378.5                 0.527046   \n",
       "\n",
       "    vloose_pd_op_total_mean  vloose_pd_op_total_std  vloose_fa_op_total_mean  \\\n",
       "0                    4916.0                     0.0                 332468.0   \n",
       "1                    3008.0                     0.0                 253818.0   \n",
       "2                    1155.0                     0.0                  48521.0   \n",
       "3                    4555.0                     0.0                 331654.0   \n",
       "4                     472.0                     0.0                  40642.0   \n",
       "5                   32722.0                     0.0                 168165.0   \n",
       "6                   19264.0                     0.0                 129473.0   \n",
       "7                    6033.0                     0.0                  21806.0   \n",
       "8                   22526.0                     0.0                 158040.0   \n",
       "9                    2162.0                     0.0                  19039.0   \n",
       "10                  87257.0                     0.0                  34622.0   \n",
       "11                  53175.0                     0.0                  27420.0   \n",
       "12                  13949.0                     0.0                   5735.0   \n",
       "13                  48453.0                     0.0                  30217.0   \n",
       "14                   4217.0                     0.0                   3333.0   \n",
       "15                  95638.0                     0.0                  10229.0   \n",
       "16                  60035.0                     0.0                   8045.0   \n",
       "17                  11128.0                     0.0                   1788.0   \n",
       "18                  53986.0                     0.0                   8111.0   \n",
       "19                   4485.0                     0.0                    829.0   \n",
       "20                  35377.0                     0.0                   5141.0   \n",
       "21                  22447.0                     0.0                   4032.0   \n",
       "22                   4038.0                     0.0                    978.0   \n",
       "23                  19224.0                     0.0                   3437.0   \n",
       "24                   1501.0                     0.0                    317.0   \n",
       "25                 226243.0                     0.0                 187639.0   \n",
       "26                 136848.0                     0.0                 143657.0   \n",
       "27                  50009.0                     0.0                  30037.0   \n",
       "28                 220933.0                     0.0                 205792.0   \n",
       "29                  19330.0                     0.0                  15512.0   \n",
       "..                      ...                     ...                      ...   \n",
       "45                1650658.0                     0.0                3769656.0   \n",
       "46                1029912.0                     0.0                2560472.0   \n",
       "47                 165962.0                     0.0                 560953.0   \n",
       "48                 875458.0                     0.0                1318418.0   \n",
       "49                  87254.0                     0.0                  93785.0   \n",
       "50                 226243.0                     0.0                 187639.0   \n",
       "51                 136848.0                     0.0                 143657.0   \n",
       "52                  50009.0                     0.0                  30037.0   \n",
       "53                 220933.0                     0.0                 205792.0   \n",
       "54                  19330.0                     0.0                  15512.0   \n",
       "55                1457047.0                     0.0                 316581.0   \n",
       "56                 871041.0                     0.0                 227709.0   \n",
       "57                 275404.0                     0.0                  47550.0   \n",
       "58                1099898.0                     0.0                 307253.0   \n",
       "59                  81560.0                     0.0                  36522.0   \n",
       "60                4069496.0                     0.0                 223869.0   \n",
       "61                2466783.0                     0.0                 156992.0   \n",
       "62                 626396.0                     0.0                  35206.0   \n",
       "63                2309655.0                     0.0                 196711.0   \n",
       "64                 159644.0                     0.0                  24980.0   \n",
       "65                4495201.0                     0.0                 162421.0   \n",
       "66                2756849.0                     0.0                 112229.0   \n",
       "67                 457627.0                     0.0                  25449.0   \n",
       "68                2360357.0                     0.0                 131668.0   \n",
       "69                 182235.0                     0.0                  16100.0   \n",
       "70                1650658.0                     0.0                3769656.0   \n",
       "71                1029912.0                     0.0                2560472.0   \n",
       "72                 165962.0                     0.0                 560953.0   \n",
       "73                 875458.0                     0.0                1318418.0   \n",
       "74                  87254.0                     0.0                  93785.0   \n",
       "\n",
       "    vloose_fa_op_total_std  \n",
       "0                      0.0  \n",
       "1                      0.0  \n",
       "2                      0.0  \n",
       "3                      0.0  \n",
       "4                      0.0  \n",
       "5                      0.0  \n",
       "6                      0.0  \n",
       "7                      0.0  \n",
       "8                      0.0  \n",
       "9                      0.0  \n",
       "10                     0.0  \n",
       "11                     0.0  \n",
       "12                     0.0  \n",
       "13                     0.0  \n",
       "14                     0.0  \n",
       "15                     0.0  \n",
       "16                     0.0  \n",
       "17                     0.0  \n",
       "18                     0.0  \n",
       "19                     0.0  \n",
       "20                     0.0  \n",
       "21                     0.0  \n",
       "22                     0.0  \n",
       "23                     0.0  \n",
       "24                     0.0  \n",
       "25                     0.0  \n",
       "26                     0.0  \n",
       "27                     0.0  \n",
       "28                     0.0  \n",
       "29                     0.0  \n",
       "..                     ...  \n",
       "45                     0.0  \n",
       "46                     0.0  \n",
       "47                     0.0  \n",
       "48                     0.0  \n",
       "49                     0.0  \n",
       "50                     0.0  \n",
       "51                     0.0  \n",
       "52                     0.0  \n",
       "53                     0.0  \n",
       "54                     0.0  \n",
       "55                     0.0  \n",
       "56                     0.0  \n",
       "57                     0.0  \n",
       "58                     0.0  \n",
       "59                     0.0  \n",
       "60                     0.0  \n",
       "61                     0.0  \n",
       "62                     0.0  \n",
       "63                     0.0  \n",
       "64                     0.0  \n",
       "65                     0.0  \n",
       "66                     0.0  \n",
       "67                     0.0  \n",
       "68                     0.0  \n",
       "69                     0.0  \n",
       "70                     0.0  \n",
       "71                     0.0  \n",
       "72                     0.0  \n",
       "73                     0.0  \n",
       "74                     0.0  \n",
       "\n",
       "[75 rows x 115 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_v10.describe(best_inits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the beamer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0730 01:23:08.021392 140735835288448 BeamerAPI.py:475] Started creating beamer file tuning_v10_tight.pdf latex code...\n",
      "I0730 01:23:15.653999 140735835288448 BeamerAPI.py:475] Started creating beamer file tuning_v10_medium.pdf latex code...\n",
      "I0730 01:23:22.596184 140735835288448 BeamerAPI.py:475] Started creating beamer file tuning_v10_loose.pdf latex code...\n",
      "I0730 01:23:29.399542 140735835288448 BeamerAPI.py:475] Started creating beamer file tuning_v10_vloose.pdf latex code...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-30 01:23:08,021 | Py.BeamerTexReportTemplate1             INFO Started creating beamer file tuning_v10_tight.pdf latex code...\n",
      "2020-07-30 01:23:15,653 | Py.BeamerTexReportTemplate1             INFO Started creating beamer file tuning_v10_medium.pdf latex code...\n",
      "2020-07-30 01:23:22,596 | Py.BeamerTexReportTemplate1             INFO Started creating beamer file tuning_v10_loose.pdf latex code...\n",
      "2020-07-30 01:23:29,399 | Py.BeamerTexReportTemplate1             INFO Started creating beamer file tuning_v10_vloose.pdf latex code...\n"
     ]
    }
   ],
   "source": [
    "# Create beamer presentation\n",
    "for op in ['tight','medium','loose','vloose']:\n",
    "    cv_v10.dump_beamer_table( best_inits ,  [15,20,25,30,40,50], \n",
    "                     [0, 0.8 , 1.37, 1.54, 2.37, 2.5], \n",
    "                     [op],\n",
    "                     'tuning_v10_'+op,\n",
    "                     title = op+' Tunings (v10)',\n",
    "                     tags = ['v7', 'v8','v10'],\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all Training for each sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_evolution_best_config_et0_eta0.pdf\n",
      "train_evolution_best_config_et0_eta1.pdf\n",
      "train_evolution_best_config_et0_eta2.pdf\n",
      "train_evolution_best_config_et0_eta3.pdf\n",
      "train_evolution_best_config_et0_eta4.pdf\n",
      "train_evolution_best_config_et1_eta0.pdf\n",
      "train_evolution_best_config_et1_eta1.pdf\n",
      "train_evolution_best_config_et1_eta2.pdf\n",
      "train_evolution_best_config_et1_eta3.pdf\n",
      "train_evolution_best_config_et1_eta4.pdf\n",
      "train_evolution_best_config_et2_eta0.pdf\n",
      "train_evolution_best_config_et2_eta1.pdf\n",
      "train_evolution_best_config_et2_eta2.pdf\n",
      "train_evolution_best_config_et2_eta3.pdf\n",
      "train_evolution_best_config_et2_eta4.pdf\n",
      "train_evolution_best_config_et3_eta0.pdf\n",
      "train_evolution_best_config_et3_eta1.pdf\n",
      "train_evolution_best_config_et3_eta2.pdf\n",
      "train_evolution_best_config_et3_eta3.pdf\n",
      "train_evolution_best_config_et3_eta4.pdf\n",
      "train_evolution_best_config_et4_eta0.pdf\n",
      "train_evolution_best_config_et4_eta1.pdf\n",
      "train_evolution_best_config_et4_eta2.pdf\n",
      "train_evolution_best_config_et4_eta3.pdf\n",
      "train_evolution_best_config_et4_eta4.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_training_curves_for_each_sort(table, et_bin, eta_bin, output, display=False):\n",
    "    \n",
    "    table = table.loc[(table.et_bin==et_bin) & (table.eta_bin==eta_bin)] \n",
    "    nsorts = len(table.sort.unique())\n",
    "    fig, ax = plt.subplots(nsorts,2, figsize=(15,20))\n",
    "    fig.suptitle(r'Monitoring Train Plot - Et = %d, Eta = %d'%(et_bin,eta_bin), fontsize=15)\n",
    "    for idx, sort in enumerate(table.sort.unique()):\n",
    "        current_table = table.loc[table.sort==sort]\n",
    "        path=current_table.file_name.values[0]\n",
    "        history = load(path)['tunedData'][current_table.model_idx.values[0]]['history']\n",
    "        best_epoch = history['max_sp_best_epoch_val'][-1]\n",
    "        # Make the plot here\n",
    "        ax[idx, 0].set_xlabel('Epochs')\n",
    "        ax[idx, 0].set_ylabel('Loss')\n",
    "        ax[idx, 0].plot(history['loss'], c='b', label='Train Step')\n",
    "        ax[idx, 0].plot(history['val_loss'], c='r', label='Validation Step') \n",
    "        ax[idx, 0].axvline(x=best_epoch, c='k', label='Best epoch')\n",
    "        ax[idx, 0].legend()\n",
    "        ax[idx, 0].grid()\n",
    "        ax[idx, 1].set_xlabel('Epochs')\n",
    "        ax[idx, 1].set_ylabel('SP')\n",
    "        ax[idx, 1].plot(history['max_sp_val'], c='r', label='Validation Step') \n",
    "        ax[idx, 1].axvline(x=best_epoch, c='k', label='Best epoch')\n",
    "        ax[idx, 1].legend()\n",
    "        ax[idx, 1].grid()\n",
    "    \n",
    "    print(output)\n",
    "    plt.savefig(output)\n",
    "    if display:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "    \n",
    "for et_bin in best_inits.et_bin.unique():\n",
    "    for eta_bin in best_inits.eta_bin.unique():\n",
    "        plot_training_curves_for_each_sort(best_inits.loc[best_inits.train_tag=='v10'],et_bin, eta_bin, 'train_evolution_best_config_et%d_eta%d.pdf'%(et_bin,eta_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the integrate value:\n",
    "\n",
    "Here we will check the integrated value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tight_pd_ref</th>\n",
       "      <th>tight_fa_ref</th>\n",
       "      <th>tight_pd_val</th>\n",
       "      <th>tight_fa_val</th>\n",
       "      <th>tight_pd_op</th>\n",
       "      <th>tight_fa_op</th>\n",
       "      <th>medium_pd_ref</th>\n",
       "      <th>medium_fa_ref</th>\n",
       "      <th>medium_pd_val</th>\n",
       "      <th>medium_fa_val</th>\n",
       "      <th>...</th>\n",
       "      <th>loose_pd_val</th>\n",
       "      <th>loose_fa_val</th>\n",
       "      <th>loose_pd_op</th>\n",
       "      <th>loose_fa_op</th>\n",
       "      <th>vloose_pd_ref</th>\n",
       "      <th>vloose_fa_ref</th>\n",
       "      <th>vloose_pd_val</th>\n",
       "      <th>vloose_fa_val</th>\n",
       "      <th>vloose_pd_op</th>\n",
       "      <th>vloose_fa_op</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>99.04946</td>\n",
       "      <td>25.621525</td>\n",
       "      <td>99.048973</td>\n",
       "      <td>0.636078</td>\n",
       "      <td>99.049485</td>\n",
       "      <td>0.570007</td>\n",
       "      <td>99.185367</td>\n",
       "      <td>2.922653e+01</td>\n",
       "      <td>99.184091</td>\n",
       "      <td>0.690721</td>\n",
       "      <td>...</td>\n",
       "      <td>99.622392</td>\n",
       "      <td>1.167215</td>\n",
       "      <td>99.621915</td>\n",
       "      <td>1.014587</td>\n",
       "      <td>99.666488</td>\n",
       "      <td>78.680117</td>\n",
       "      <td>99.666150</td>\n",
       "      <td>1.296809</td>\n",
       "      <td>99.666455</td>\n",
       "      <td>1.092690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.851389e-15</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.028836</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.059423</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.015164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tight_pd_ref  tight_fa_ref  tight_pd_val  tight_fa_val  tight_pd_op  \\\n",
       "mean      99.04946     25.621525     99.048973      0.636078    99.049485   \n",
       "std        0.00000      0.000000      0.003152      0.011599     0.000196   \n",
       "\n",
       "      tight_fa_op  medium_pd_ref  medium_fa_ref  medium_pd_val  medium_fa_val  \\\n",
       "mean     0.570007      99.185367   2.922653e+01      99.184091       0.690721   \n",
       "std      0.006661       0.000000   5.851389e-15       0.002094       0.010400   \n",
       "\n",
       "      ...  loose_pd_val  loose_fa_val  loose_pd_op  loose_fa_op  \\\n",
       "mean  ...     99.622392      1.167215    99.621915     1.014587   \n",
       "std   ...      0.002829      0.028836     0.000129     0.013568   \n",
       "\n",
       "      vloose_pd_ref  vloose_fa_ref  vloose_pd_val  vloose_fa_val  \\\n",
       "mean      99.666488      78.680117      99.666150       1.296809   \n",
       "std        0.000000       0.000000       0.002425       0.059423   \n",
       "\n",
       "      vloose_pd_op  vloose_fa_op  \n",
       "mean     99.666455      1.092690  \n",
       "std       0.000230      0.015164  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_v10.integrate(best_inits, 'v10') * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Best sort for all phase spaces:\n",
    "\n",
    "\n",
    "Here we will access the best sort for each phase space. We can use this information to export each tuning in each phase space to a unique file that will be used in the pileup adjustiment step (using prometheus framework). We will get the best sort with the highest max SP value for each phase space always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 98)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_tag</th>\n",
       "      <th>et_bin</th>\n",
       "      <th>eta_bin</th>\n",
       "      <th>model_idx</th>\n",
       "      <th>sort</th>\n",
       "      <th>init</th>\n",
       "      <th>file_name</th>\n",
       "      <th>tuned_idx</th>\n",
       "      <th>max_sp_val</th>\n",
       "      <th>max_sp_pd_val</th>\n",
       "      <th>...</th>\n",
       "      <th>vloose_pd_ref_total</th>\n",
       "      <th>vloose_fa_ref_total</th>\n",
       "      <th>vloose_pd_val_passed</th>\n",
       "      <th>vloose_fa_val_passed</th>\n",
       "      <th>vloose_pd_val_total</th>\n",
       "      <th>vloose_fa_val_total</th>\n",
       "      <th>vloose_pd_op_passed</th>\n",
       "      <th>vloose_fa_op_passed</th>\n",
       "      <th>vloose_pd_op_total</th>\n",
       "      <th>vloose_fa_op_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980974</td>\n",
       "      <td>0.986961</td>\n",
       "      <td>...</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "      <td>22365</td>\n",
       "      <td>514</td>\n",
       "      <td>22624</td>\n",
       "      <td>18764</td>\n",
       "      <td>223651</td>\n",
       "      <td>5090</td>\n",
       "      <td>226243</td>\n",
       "      <td>187639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972888</td>\n",
       "      <td>0.981804</td>\n",
       "      <td>...</td>\n",
       "      <td>136848</td>\n",
       "      <td>143657</td>\n",
       "      <td>13518</td>\n",
       "      <td>656</td>\n",
       "      <td>13684</td>\n",
       "      <td>14366</td>\n",
       "      <td>135180</td>\n",
       "      <td>5757</td>\n",
       "      <td>136848</td>\n",
       "      <td>143657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949986</td>\n",
       "      <td>0.959608</td>\n",
       "      <td>...</td>\n",
       "      <td>50009</td>\n",
       "      <td>30037</td>\n",
       "      <td>4881</td>\n",
       "      <td>343</td>\n",
       "      <td>5001</td>\n",
       "      <td>3004</td>\n",
       "      <td>48805</td>\n",
       "      <td>2937</td>\n",
       "      <td>50009</td>\n",
       "      <td>30037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973435</td>\n",
       "      <td>0.984837</td>\n",
       "      <td>...</td>\n",
       "      <td>220933</td>\n",
       "      <td>205792</td>\n",
       "      <td>21711</td>\n",
       "      <td>740</td>\n",
       "      <td>22093</td>\n",
       "      <td>20580</td>\n",
       "      <td>217078</td>\n",
       "      <td>7092</td>\n",
       "      <td>220933</td>\n",
       "      <td>205792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>v10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952540</td>\n",
       "      <td>0.958614</td>\n",
       "      <td>...</td>\n",
       "      <td>19330</td>\n",
       "      <td>15512</td>\n",
       "      <td>1812</td>\n",
       "      <td>67</td>\n",
       "      <td>1933</td>\n",
       "      <td>1551</td>\n",
       "      <td>18083</td>\n",
       "      <td>734</td>\n",
       "      <td>19330</td>\n",
       "      <td>15512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988932</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>...</td>\n",
       "      <td>1457047</td>\n",
       "      <td>316581</td>\n",
       "      <td>144971</td>\n",
       "      <td>573</td>\n",
       "      <td>145705</td>\n",
       "      <td>31658</td>\n",
       "      <td>1449715</td>\n",
       "      <td>5801</td>\n",
       "      <td>1457047</td>\n",
       "      <td>316581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>0.988715</td>\n",
       "      <td>...</td>\n",
       "      <td>871041</td>\n",
       "      <td>227709</td>\n",
       "      <td>86774</td>\n",
       "      <td>870</td>\n",
       "      <td>87104</td>\n",
       "      <td>22771</td>\n",
       "      <td>867739</td>\n",
       "      <td>8338</td>\n",
       "      <td>871041</td>\n",
       "      <td>227709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973211</td>\n",
       "      <td>0.978613</td>\n",
       "      <td>...</td>\n",
       "      <td>275404</td>\n",
       "      <td>47550</td>\n",
       "      <td>26669</td>\n",
       "      <td>125</td>\n",
       "      <td>27540</td>\n",
       "      <td>4755</td>\n",
       "      <td>266304</td>\n",
       "      <td>1307</td>\n",
       "      <td>275404</td>\n",
       "      <td>47550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.981608</td>\n",
       "      <td>0.989645</td>\n",
       "      <td>...</td>\n",
       "      <td>1099898</td>\n",
       "      <td>307253</td>\n",
       "      <td>109515</td>\n",
       "      <td>1159</td>\n",
       "      <td>109990</td>\n",
       "      <td>30725</td>\n",
       "      <td>1095157</td>\n",
       "      <td>11329</td>\n",
       "      <td>1099898</td>\n",
       "      <td>307253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972431</td>\n",
       "      <td>0.984919</td>\n",
       "      <td>...</td>\n",
       "      <td>81560</td>\n",
       "      <td>36522</td>\n",
       "      <td>7817</td>\n",
       "      <td>105</td>\n",
       "      <td>8156</td>\n",
       "      <td>3652</td>\n",
       "      <td>78081</td>\n",
       "      <td>1087</td>\n",
       "      <td>81560</td>\n",
       "      <td>36522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993737</td>\n",
       "      <td>0.995963</td>\n",
       "      <td>...</td>\n",
       "      <td>4069496</td>\n",
       "      <td>223869</td>\n",
       "      <td>406255</td>\n",
       "      <td>316</td>\n",
       "      <td>406949</td>\n",
       "      <td>22387</td>\n",
       "      <td>4062510</td>\n",
       "      <td>2945</td>\n",
       "      <td>4069496</td>\n",
       "      <td>223869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990252</td>\n",
       "      <td>0.994462</td>\n",
       "      <td>...</td>\n",
       "      <td>2466783</td>\n",
       "      <td>156992</td>\n",
       "      <td>246446</td>\n",
       "      <td>429</td>\n",
       "      <td>246678</td>\n",
       "      <td>15700</td>\n",
       "      <td>2464465</td>\n",
       "      <td>4335</td>\n",
       "      <td>2466783</td>\n",
       "      <td>156992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985177</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>...</td>\n",
       "      <td>626396</td>\n",
       "      <td>35206</td>\n",
       "      <td>61328</td>\n",
       "      <td>57</td>\n",
       "      <td>62639</td>\n",
       "      <td>3521</td>\n",
       "      <td>613626</td>\n",
       "      <td>539</td>\n",
       "      <td>626396</td>\n",
       "      <td>35206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988252</td>\n",
       "      <td>0.994519</td>\n",
       "      <td>...</td>\n",
       "      <td>2309655</td>\n",
       "      <td>196711</td>\n",
       "      <td>230768</td>\n",
       "      <td>696</td>\n",
       "      <td>230965</td>\n",
       "      <td>19672</td>\n",
       "      <td>2307689</td>\n",
       "      <td>6353</td>\n",
       "      <td>2309655</td>\n",
       "      <td>196711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982714</td>\n",
       "      <td>0.992295</td>\n",
       "      <td>...</td>\n",
       "      <td>159644</td>\n",
       "      <td>24980</td>\n",
       "      <td>15424</td>\n",
       "      <td>53</td>\n",
       "      <td>15964</td>\n",
       "      <td>2498</td>\n",
       "      <td>154388</td>\n",
       "      <td>486</td>\n",
       "      <td>159644</td>\n",
       "      <td>24980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995243</td>\n",
       "      <td>0.997199</td>\n",
       "      <td>...</td>\n",
       "      <td>4495201</td>\n",
       "      <td>162421</td>\n",
       "      <td>449270</td>\n",
       "      <td>225</td>\n",
       "      <td>449520</td>\n",
       "      <td>16242</td>\n",
       "      <td>4492713</td>\n",
       "      <td>2088</td>\n",
       "      <td>4495201</td>\n",
       "      <td>162421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992576</td>\n",
       "      <td>0.995491</td>\n",
       "      <td>...</td>\n",
       "      <td>2756849</td>\n",
       "      <td>112229</td>\n",
       "      <td>275611</td>\n",
       "      <td>362</td>\n",
       "      <td>275685</td>\n",
       "      <td>11223</td>\n",
       "      <td>2756114</td>\n",
       "      <td>3213</td>\n",
       "      <td>2756849</td>\n",
       "      <td>112229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.988172</td>\n",
       "      <td>0.992068</td>\n",
       "      <td>...</td>\n",
       "      <td>457627</td>\n",
       "      <td>25449</td>\n",
       "      <td>44784</td>\n",
       "      <td>25</td>\n",
       "      <td>45763</td>\n",
       "      <td>2545</td>\n",
       "      <td>447930</td>\n",
       "      <td>308</td>\n",
       "      <td>457627</td>\n",
       "      <td>25449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990736</td>\n",
       "      <td>0.996221</td>\n",
       "      <td>...</td>\n",
       "      <td>2360357</td>\n",
       "      <td>131668</td>\n",
       "      <td>235960</td>\n",
       "      <td>419</td>\n",
       "      <td>236035</td>\n",
       "      <td>13167</td>\n",
       "      <td>2359619</td>\n",
       "      <td>4359</td>\n",
       "      <td>2360357</td>\n",
       "      <td>131668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990683</td>\n",
       "      <td>0.995665</td>\n",
       "      <td>...</td>\n",
       "      <td>182235</td>\n",
       "      <td>16100</td>\n",
       "      <td>17728</td>\n",
       "      <td>15</td>\n",
       "      <td>18223</td>\n",
       "      <td>1610</td>\n",
       "      <td>176961</td>\n",
       "      <td>215</td>\n",
       "      <td>182235</td>\n",
       "      <td>16100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998695</td>\n",
       "      <td>0.999315</td>\n",
       "      <td>...</td>\n",
       "      <td>1650658</td>\n",
       "      <td>3769656</td>\n",
       "      <td>165026</td>\n",
       "      <td>1462</td>\n",
       "      <td>165066</td>\n",
       "      <td>376965</td>\n",
       "      <td>1650261</td>\n",
       "      <td>11511</td>\n",
       "      <td>1650658</td>\n",
       "      <td>3769656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997669</td>\n",
       "      <td>0.998932</td>\n",
       "      <td>...</td>\n",
       "      <td>1029912</td>\n",
       "      <td>2560472</td>\n",
       "      <td>102916</td>\n",
       "      <td>1083</td>\n",
       "      <td>102991</td>\n",
       "      <td>256047</td>\n",
       "      <td>1029154</td>\n",
       "      <td>9782</td>\n",
       "      <td>1029912</td>\n",
       "      <td>2560472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997220</td>\n",
       "      <td>0.997951</td>\n",
       "      <td>...</td>\n",
       "      <td>165962</td>\n",
       "      <td>560953</td>\n",
       "      <td>16359</td>\n",
       "      <td>65</td>\n",
       "      <td>16596</td>\n",
       "      <td>56096</td>\n",
       "      <td>163636</td>\n",
       "      <td>443</td>\n",
       "      <td>165962</td>\n",
       "      <td>560953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994962</td>\n",
       "      <td>0.996653</td>\n",
       "      <td>...</td>\n",
       "      <td>875458</td>\n",
       "      <td>1318418</td>\n",
       "      <td>87531</td>\n",
       "      <td>3614</td>\n",
       "      <td>87546</td>\n",
       "      <td>131842</td>\n",
       "      <td>875308</td>\n",
       "      <td>22046</td>\n",
       "      <td>875458</td>\n",
       "      <td>1318418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>/Users/jodafons/Desktop/phd_local/prometheus/a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994380</td>\n",
       "      <td>0.996332</td>\n",
       "      <td>...</td>\n",
       "      <td>87254</td>\n",
       "      <td>93785</td>\n",
       "      <td>8437</td>\n",
       "      <td>31</td>\n",
       "      <td>8725</td>\n",
       "      <td>9379</td>\n",
       "      <td>83739</td>\n",
       "      <td>311</td>\n",
       "      <td>87254</td>\n",
       "      <td>93785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_tag  et_bin  eta_bin  model_idx  sort  init  \\\n",
       "19        v10       0        0          0     4     1   \n",
       "32        v10       0        1          0     9     0   \n",
       "42        v10       0        2          0     2     1   \n",
       "66        v10       0        3          0     3     1   \n",
       "82        v10       0        4          0     2     1   \n",
       "106       v10       1        0          0     3     1   \n",
       "135       v10       1        1          0     3     0   \n",
       "145       v10       1        2          0     8     1   \n",
       "177       v10       1        3          0     1     0   \n",
       "190       v10       1        4          0     7     0   \n",
       "210       v10       2        0          0     7     0   \n",
       "239       v10       2        1          0     4     1   \n",
       "254       v10       2        2          0     6     0   \n",
       "261       v10       2        3          0     5     1   \n",
       "288       v10       2        4          0     7     1   \n",
       "319       v10       3        0          0     4     1   \n",
       "334       v10       3        1          0     6     0   \n",
       "349       v10       3        2          0     1     1   \n",
       "372       v10       3        3          0     9     0   \n",
       "396       v10       3        4          0     6     1   \n",
       "419       v10       4        0          0     4     1   \n",
       "425       v10       4        1          0     8     1   \n",
       "440       v10       4        2          0     4     0   \n",
       "467       v10       4        3          0     0     0   \n",
       "496       v10       4        4          0     6     1   \n",
       "\n",
       "                                             file_name  tuned_idx  max_sp_val  \\\n",
       "19   /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.980974   \n",
       "32   /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.972888   \n",
       "42   /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.949986   \n",
       "66   /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.973435   \n",
       "82   /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.952540   \n",
       "106  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.988932   \n",
       "135  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.982953   \n",
       "145  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.973211   \n",
       "177  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.981608   \n",
       "190  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.972431   \n",
       "210  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.993737   \n",
       "239  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.990252   \n",
       "254  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.985177   \n",
       "261  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.988252   \n",
       "288  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.982714   \n",
       "319  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.995243   \n",
       "334  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.992576   \n",
       "349  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.988172   \n",
       "372  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.990736   \n",
       "396  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.990683   \n",
       "419  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.998695   \n",
       "425  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.997669   \n",
       "440  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.997220   \n",
       "467  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.994962   \n",
       "496  /Users/jodafons/Desktop/phd_local/prometheus/a...          0    0.994380   \n",
       "\n",
       "     max_sp_pd_val  ...  vloose_pd_ref_total  vloose_fa_ref_total  \\\n",
       "19        0.986961  ...               226243               187639   \n",
       "32        0.981804  ...               136848               143657   \n",
       "42        0.959608  ...                50009                30037   \n",
       "66        0.984837  ...               220933               205792   \n",
       "82        0.958614  ...                19330                15512   \n",
       "106       0.993384  ...              1457047               316581   \n",
       "135       0.988715  ...               871041               227709   \n",
       "145       0.978613  ...               275404                47550   \n",
       "177       0.989645  ...              1099898               307253   \n",
       "190       0.984919  ...                81560                36522   \n",
       "210       0.995963  ...              4069496               223869   \n",
       "239       0.994462  ...              2466783               156992   \n",
       "254       0.990533  ...               626396                35206   \n",
       "261       0.994519  ...              2309655               196711   \n",
       "288       0.992295  ...               159644                24980   \n",
       "319       0.997199  ...              4495201               162421   \n",
       "334       0.995491  ...              2756849               112229   \n",
       "349       0.992068  ...               457627                25449   \n",
       "372       0.996221  ...              2360357               131668   \n",
       "396       0.995665  ...               182235                16100   \n",
       "419       0.999315  ...              1650658              3769656   \n",
       "425       0.998932  ...              1029912              2560472   \n",
       "440       0.997951  ...               165962               560953   \n",
       "467       0.996653  ...               875458              1318418   \n",
       "496       0.996332  ...                87254                93785   \n",
       "\n",
       "     vloose_pd_val_passed  vloose_fa_val_passed  vloose_pd_val_total  \\\n",
       "19                  22365                   514                22624   \n",
       "32                  13518                   656                13684   \n",
       "42                   4881                   343                 5001   \n",
       "66                  21711                   740                22093   \n",
       "82                   1812                    67                 1933   \n",
       "106                144971                   573               145705   \n",
       "135                 86774                   870                87104   \n",
       "145                 26669                   125                27540   \n",
       "177                109515                  1159               109990   \n",
       "190                  7817                   105                 8156   \n",
       "210                406255                   316               406949   \n",
       "239                246446                   429               246678   \n",
       "254                 61328                    57                62639   \n",
       "261                230768                   696               230965   \n",
       "288                 15424                    53                15964   \n",
       "319                449270                   225               449520   \n",
       "334                275611                   362               275685   \n",
       "349                 44784                    25                45763   \n",
       "372                235960                   419               236035   \n",
       "396                 17728                    15                18223   \n",
       "419                165026                  1462               165066   \n",
       "425                102916                  1083               102991   \n",
       "440                 16359                    65                16596   \n",
       "467                 87531                  3614                87546   \n",
       "496                  8437                    31                 8725   \n",
       "\n",
       "     vloose_fa_val_total  vloose_pd_op_passed  vloose_fa_op_passed  \\\n",
       "19                 18764               223651                 5090   \n",
       "32                 14366               135180                 5757   \n",
       "42                  3004                48805                 2937   \n",
       "66                 20580               217078                 7092   \n",
       "82                  1551                18083                  734   \n",
       "106                31658              1449715                 5801   \n",
       "135                22771               867739                 8338   \n",
       "145                 4755               266304                 1307   \n",
       "177                30725              1095157                11329   \n",
       "190                 3652                78081                 1087   \n",
       "210                22387              4062510                 2945   \n",
       "239                15700              2464465                 4335   \n",
       "254                 3521               613626                  539   \n",
       "261                19672              2307689                 6353   \n",
       "288                 2498               154388                  486   \n",
       "319                16242              4492713                 2088   \n",
       "334                11223              2756114                 3213   \n",
       "349                 2545               447930                  308   \n",
       "372                13167              2359619                 4359   \n",
       "396                 1610               176961                  215   \n",
       "419               376965              1650261                11511   \n",
       "425               256047              1029154                 9782   \n",
       "440                56096               163636                  443   \n",
       "467               131842               875308                22046   \n",
       "496                 9379                83739                  311   \n",
       "\n",
       "     vloose_pd_op_total  vloose_fa_op_total  \n",
       "19               226243              187639  \n",
       "32               136848              143657  \n",
       "42                50009               30037  \n",
       "66               220933              205792  \n",
       "82                19330               15512  \n",
       "106             1457047              316581  \n",
       "135              871041              227709  \n",
       "145              275404               47550  \n",
       "177             1099898              307253  \n",
       "190               81560               36522  \n",
       "210             4069496              223869  \n",
       "239             2466783              156992  \n",
       "254              626396               35206  \n",
       "261             2309655              196711  \n",
       "288              159644               24980  \n",
       "319             4495201              162421  \n",
       "334             2756849              112229  \n",
       "349              457627               25449  \n",
       "372             2360357              131668  \n",
       "396              182235               16100  \n",
       "419             1650658             3769656  \n",
       "425             1029912             2560472  \n",
       "440              165962              560953  \n",
       "467              875458             1318418  \n",
       "496               87254               93785  \n",
       "\n",
       "[25 rows x 98 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sorts = cv_v10.filter_sorts(best_inits_v10, 'max_sp_val')\n",
    "print(best_sorts.shape)\n",
    "best_sorts.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump all best tunings:\n",
    "\n",
    "Here we will dump all 20 configuration into a json file. We use the same neural netowrks for each operation point. Only the thresholds are variated. The threshold correction (alpha and beta) and files will be calculated/generated using the pileup correction tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "Reshape_layer (Reshape)      (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_layer_1 (Conv1D)      (None, 99, 16)            48        \n",
      "_________________________________________________________________\n",
      "conv1d_layer_2 (Conv1D)      (None, 98, 32)            1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 32)                100384    \n",
      "_________________________________________________________________\n",
      "output_for_inference (Dense) (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 101,521\n",
      "Trainable params: 101,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_best_models( best_sorts , etbins, etabins):\n",
    "\n",
    "    from saphyra.layers.RpLayer import RpLayer\n",
    "    # Just to remove the keras dependence\n",
    "    import tensorflow as tf\n",
    "    model_from_json = tf.keras.models.model_from_json\n",
    "    import json\n",
    "    \n",
    "    models = []\n",
    "\n",
    "    for et_bin in range(len(etbins)-1):\n",
    "        for eta_bin in range(len(etabins)-1):\n",
    "            d_tuned = {}\n",
    "            best = best_sorts.loc[(best_sorts.et_bin==et_bin) & (best_sorts.eta_bin==eta_bin)]\n",
    "            best.head()\n",
    "            tuned = load(best.file_name.values[0])['tunedData'][best.model_idx.values[0]]\n",
    "            model = model_from_json( json.dumps(tuned['sequence'], separators=(',', ':')) , \n",
    "                                 custom_objects={'RpLayer':RpLayer} )\n",
    "            model.set_weights( tuned['weights'] )\n",
    "            \n",
    "            model._layers.pop()\n",
    "            model.summary()\n",
    "            \n",
    "            \n",
    "            d_tuned['model']    = model\n",
    "            d_tuned['etBin']    = [etbins[et_bin], etbins[et_bin+1]]\n",
    "            d_tuned['etaBin']   = [etabins[eta_bin], etabins[eta_bin+1]]\n",
    "            d_tuned['etBinIdx'] = et_bin\n",
    "            d_tuned['etaBinIdx']= eta_bin\n",
    "            models.append(d_tuned)\n",
    "    return models\n",
    "            \n",
    "        \n",
    "    \n",
    "etbins = [15,20,30,40,50,100000]\n",
    "etabins = [0, 0.8 , 1.37, 1.54, 2.37, 2.5]\n",
    "\n",
    "\n",
    "models = get_best_models( best_sorts, etbins, etabins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump to onnx format:\n",
    "\n",
    "We will dump all best models using dummy thresholds. This threshold should be rewrited using the pileup correction tool provided by the prometheus framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.405292 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.407180 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.456341 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.461817 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.469162 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.471841 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.519776 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.525022 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.531919 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.534641 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.581358 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.586442 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.593273 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.595365 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.643740 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.648844 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.655484 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.658817 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.706746 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.712318 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.717691 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.719558 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.767640 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.772716 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.778462 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.780332 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.829562 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.835283 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.841329 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.843369 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.892703 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.898375 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.904862 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.906895 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:34.958479 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:34.963577 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:34.971001 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:34.972724 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.026297 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.031845 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.038533 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.040450 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.090229 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.095986 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.103301 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.105504 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.153874 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.158963 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.165658 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.167721 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.215056 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.221076 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.227450 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.229449 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.282294 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.288251 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.295485 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.297374 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.343830 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.349348 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.354646 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.356436 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.408424 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.413477 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.421379 140735835288448 main.py:44] tf executing eager_mode: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.423253 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.470265 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.475558 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.481271 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.483763 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.534593 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.541846 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.547081 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.551429 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.601181 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.606785 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.612961 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.615425 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.669031 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.674782 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.681427 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.683525 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.727951 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.733306 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.739787 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.742244 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.792053 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.797163 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.803152 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.805533 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.852742 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.858503 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.864164 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.867072 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.916090 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.921557 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:35.927371 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:35.930619 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:35.977624 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:35.983662 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.016920 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.019475 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.073700 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.078963 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.085743 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.087968 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.136078 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.141845 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.148566 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.150849 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.199373 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.204384 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.210638 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.213280 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.264283 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.270172 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.275553 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.277523 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.328694 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.334514 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.341022 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.343515 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.396296 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.401340 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.407896 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.410058 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.461591 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.467371 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.473028 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.475183 140735835288448 main.py:46] tf.keras model eager_mode: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.524902 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.530216 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.536679 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.538967 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.586142 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.591141 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.598196 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.600794 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.656044 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.661733 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.668069 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.670295 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.718496 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.723455 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.729444 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.732077 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.783814 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.788974 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.794766 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.797631 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.852179 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.857701 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.863887 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.865844 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.916356 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.922309 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.928493 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.930701 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:36.978440 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:36.983781 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:36.989227 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:36.991397 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.040843 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.046522 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.052105 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.054635 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.102709 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.108067 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.113509 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.115628 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.168520 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.173925 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.180284 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.182354 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.233150 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.238826 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.245475 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.247935 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.300590 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.306108 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.312194 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.314433 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.365274 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.371289 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.377022 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.378904 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.428078 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.434209 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.440933 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.443279 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.490839 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.496664 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.504446 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.506789 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0730 01:24:37.554342 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.560117 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.565913 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.568403 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.616773 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.622522 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.630242 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.632164 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.688259 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.694042 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.699942 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.702183 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.750895 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.756416 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.762668 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.765695 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.816278 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.821789 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.827934 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.830137 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.880195 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.885648 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.892652 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.895092 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:37.943217 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:37.948189 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:37.954458 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:37.956869 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.005635 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.011649 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.017743 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.020148 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.068298 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.073809 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.080141 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.085237 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.133395 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.139067 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.146810 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.148922 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.199775 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.205229 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.211762 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.214376 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.267669 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.273653 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.280014 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.282744 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.334925 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.341374 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.347699 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.350326 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.401255 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.407400 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.413008 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.416323 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.466829 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.473309 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.479417 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.482181 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.529338 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.535084 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.541672 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.544432 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.595556 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0730 01:24:38.601199 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.607749 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.610275 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.659491 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.665402 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.671103 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.673591 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.724418 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.730042 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.735739 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.737746 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.788401 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.793615 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.799447 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.801609 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.851459 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.856728 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.862351 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.865434 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.916942 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.922409 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.928849 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.931090 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:38.978544 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:38.984570 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:38.990203 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:38.992432 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.041392 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.047144 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.053082 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.055419 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.102653 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.108227 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.113547 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.115674 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.167088 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.172235 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.179749 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.181862 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.233595 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.239300 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.247002 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.249275 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.304318 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.309383 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.316243 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.318799 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.365917 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.371604 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.376839 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.378835 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.429104 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.435013 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.440706 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.442669 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.494416 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.499886 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.506659 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.508883 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.558526 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.563610 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.570744 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.573194 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.619626 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.625315 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.630661 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.632922 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.679224 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.685298 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.691112 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.693562 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.742051 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.747051 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.753458 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.755707 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.805955 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.811132 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.816655 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.819193 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.874511 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.880136 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.886579 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.888548 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:39.939281 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:39.945029 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:39.950638 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:39.952775 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.001934 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.007426 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.013475 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.015952 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.065387 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.070746 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.077888 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.080463 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.129270 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.135252 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.140838 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.142890 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.191411 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.196878 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.202703 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.205656 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.254118 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.260724 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.267963 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.270679 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.322063 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.327361 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.333781 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.336002 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.390207 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.395633 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.401767 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.403866 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.452490 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.458143 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.464068 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.465978 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.515194 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.520596 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.526966 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.529095 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.579581 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.585931 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.593082 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.595528 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.646378 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.652494 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.658613 140735835288448 main.py:44] tf executing eager_mode: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.661092 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.710628 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.716466 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.722604 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.724778 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.775504 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.781157 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n",
      "tf executing eager_mode: True\n",
      "I0730 01:24:40.787529 140735835288448 main.py:44] tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "I0730 01:24:40.789861 140735835288448 main.py:46] tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 13\n",
      "I0730 01:24:40.839704 140735835288448 topology.py:348] The ONNX operator number change on the optimization: 22 -> 13\n",
      "W0730 01:24:40.845175 140735835288448 onnx_ex.py:64] The maximum opset needed by this model is only 11.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_onnx_with_dummy_thresholds( models, name, version, signature, model_output_format , operation, output):\n",
    "\n",
    "    import onnx\n",
    "    import keras2onnx\n",
    "    from ROOT import TEnv\n",
    "    model_etmin_vec = []\n",
    "    model_etmax_vec = []\n",
    "    model_etamin_vec = []\n",
    "    model_etamax_vec = []\n",
    "    model_paths = []\n",
    "\n",
    "    slopes = []\n",
    "    offsets = []\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        model_etmin_vec.append( model['etBin'][0] )\n",
    "        model_etmax_vec.append( model['etBin'][1] )\n",
    "        model_etamin_vec.append( model['etaBin'][0] )\n",
    "        model_etamax_vec.append( model['etaBin'][1] )\n",
    "\n",
    "        etBinIdx = model['etBinIdx']\n",
    "        etaBinIdx = model['etaBinIdx']\n",
    "\n",
    "        # Conver keras to Onnx\n",
    "        onnx_model = keras2onnx.convert_keras(model['model'], model['model'].name)\n",
    "\n",
    "        onnx_model_name = model_output_format%( etBinIdx, etaBinIdx )\n",
    "        model_paths.append( onnx_model_name )\n",
    "\n",
    "        # Save onnx mode!\n",
    "        onnx.save_model(onnx_model, onnx_model_name)\n",
    "\n",
    "        slopes.append( 0.0 )\n",
    "        offsets.append( 0.0 )\n",
    "\n",
    "\n",
    "    def list_to_str( l ):\n",
    "        s = str()\n",
    "        for ll in l:\n",
    "          s+=str(ll)+'; '\n",
    "        return s[:-2]\n",
    "\n",
    "    # Write the config file\n",
    "    file = TEnv( 'ringer' )\n",
    "    file.SetValue( \"__name__\", name )\n",
    "    file.SetValue( \"__version__\", version )\n",
    "    file.SetValue( \"__operation__\", operation )\n",
    "    file.SetValue( \"__signature__\", signature )\n",
    "    file.SetValue( \"Model__size\"  , str(len(models)) )\n",
    "    file.SetValue( \"Model__etmin\" , list_to_str(model_etmin_vec) )\n",
    "    file.SetValue( \"Model__etmax\" , list_to_str(model_etmax_vec) )\n",
    "    file.SetValue( \"Model__etamin\", list_to_str(model_etamin_vec) )\n",
    "    file.SetValue( \"Model__etamax\", list_to_str(model_etamax_vec) )\n",
    "    file.SetValue( \"Model__path\"  , list_to_str( model_paths ) )\n",
    "    file.SetValue( \"Threshold__size\"  , str(len(models)) )\n",
    "    file.SetValue( \"Threshold__etmin\" , list_to_str(model_etmin_vec) )\n",
    "    file.SetValue( \"Threshold__etmax\" , list_to_str(model_etmax_vec) )\n",
    "    file.SetValue( \"Threshold__etamin\", list_to_str(model_etamin_vec) )\n",
    "    file.SetValue( \"Threshold__etamax\", list_to_str(model_etamax_vec) )\n",
    "    file.SetValue( \"Threshold__slope\" , list_to_str(slopes) )\n",
    "    file.SetValue( \"Threshold__offset\", list_to_str(offsets) )\n",
    "    file.SetValue( \"Threshold__MaxAverageMu\", 100)\n",
    "    file.WriteFile(output)\n",
    "\n",
    "\n",
    "    \n",
    "for op in ['Tight','Medium','Loose','VeryLoose']:\n",
    "\n",
    "    format = 'data17_13TeV_EGAM1_probes_lhmedium_EGAM7_vetolhvloose.model_v10.electron'+op+'.et%d_eta%d.onnx'\n",
    "    output = \"ElectronRinger%sTriggerConfig.conf\"%op\n",
    "    convert_to_onnx_with_dummy_thresholds( models, 'TrigL2_20200715_v10', 'v10', 'electron', format ,op ,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
